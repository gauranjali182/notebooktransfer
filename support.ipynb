{"metadata":{"kernelspec":{"name":"pysparkkernel","display_name":"PySpark","language":""},"language_info":{"name":"pyspark","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3"},"qubole":{"kernel_log_url":"https://us.qubole.com/jupyter-notebook-49246/qubole/api/v1/kernel_logs/4e5da81f-5ede-46cc-bf37-4e7023e469d3","session_data":{"id":60,"mode":"scoped","spark_ui_url":"/cluster-proxy?clusterInst=7044654\u0026encodedUrl=http%3A%2F%2F10.150.200.152%3A8088%2Fproxy%2Fapplication_1715832434620_0061%2F%3Fspark%3Dtrue","driver_log_url":"/cluster-proxy?clusterInst=7044654\u0026encodedUrl=http%3A%2F%2F10.150.200.4%3A8042%2Fnode%2Fcontainerlogs%2Fcontainer_1715832434620_0061_01_000001%2Fagaur"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\nimport numpy as np\nfrom pyspark.sql import SQLContext\nfrom pyspark.conf import SparkConf\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport pandasql as ps\nimport pyarrow\nimport boto3\nimport pyodbc\nimport ast\nimport io\nimport string\nimport re\nfrom datetime import date,timedelta\nfrom IPython.display import display\nimport json\nimport random\nimport time\nimport pandasql as ps\npd.options.mode.chained_assignment = None\nfrom pyspark.sql.types import IntegerType,StructType,StructField,StringType\nfrom pyspark.sql.functions import lit,array, create_map, struct\nfrom pyspark.sql.functions import col, concat, regexp_extract, when,max as max_,min as min_\nfrom pyspark.sql.functions import date_format\nfrom pyspark.sql.functions import udf,col\nfrom datetime import datetime,timedelta\nimport traceback\nfrom pymongo import MongoClient\nimport csv\nsc.addPyFile(\"s3n://c1-data-lake/ETL/ETL_Dependency/etlconfig.py\")\nsc.addPyFile(\"s3://c1-data-lake/ETL/UDF/prod/Udf.py\")\nsc.addPyFile(\"s3n://c1-data-lake/ETL/psqlconfig/psqlconfig.py\")\nimport Udf\nimport etlconfig\nimport psqlconfig\nimport pyarrow\n","metadata":{"qubole":{"import_errors":true,"execution_info":{"started_at":"2024-05-16T08:47:09.876416Z","ended_at":"2024-05-16T08:47:12.250244Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_API_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getCurrentDate(delta):\n    todaydate = date.today()-timedelta(days=delta)\n    return todaydate\ngetCurrentDate(5)\n","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T04:42:03.417958Z","ended_at":"2024-05-16T04:42:03.588971Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def getDeltaDate(delta,fileDate):\n    daltadate=datetime.strptime(fileDate, '%m-%d-%Y')\n    return daltadate.date()-timedelta(days=delta)\n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getdeltaDate(delta,file_date):\n    ddate=datetime.strptime(file_date,'%m-%d-%Y')\n    daldate=ddate.date()-timedelta(days=delta)\n    return daldate\ngetdeltaDate(2,\"02-21-2023\")\n     ","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T05:00:10.338559Z","ended_at":"2024-05-16T05:00:10.441253Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def querytoDataframe(Query):\n    #cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=10.100.11.15;DATABASE=datalake;UID=agaur;PWD=Pune@123$A')\n    cnxn = pyodbc.connect(etlconfig.pyodbc_datalake_prod)\n    querystring_details = str(Query)\n    try:\n        dataframe_details = pd.read_sql(querystring_details, cnxn)\n    except:\n        print(querystring_details)\n    cnxn.close()\n    return dataframe_details\n#querytoDataframe('select * from master.dealer')\n","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T05:43:35.716021Z","ended_at":"2024-05-16T05:43:35.831589Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"querytoDataframe('select * from master.dealer')","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T05:26:30.300522Z","ended_at":"2024-05-16T05:26:30.381423Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=10.100.11.15;DATABASE=datalake;UID=agaur;PWD=Pune@123$A')\ncnxn = pyodbc.connect(etlconfig.pyodbc_datalake_prod)\nquerystring_details = str('select * from master.dealer')\n\ndataframe_details = pd.read_sql(querystring_details, cnxn)","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T05:34:16.993085Z","ended_at":"2024-05-16T05:34:17.273197Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\n%%configure -f\n{ \"driverMemory\": \"5G\", \"conf\" : {\"spark.executor.memory\":\"5g\",\"spark.driver.memory\":\"5g\",\"spark.memory.offHeap.enabled\":\"True\",\"spark.memory.offHeap.size\":\"5g\",\"spark.driver.maxResultSize\":\"2g\",\"spark.rpc.message.maxSize\": 1500,\"spark.akka.frameSize\":64,\"spark.speculation\":\"true\",\n                                   \"spark.speculation.multiplier\":2,\"spark.speculation.quantile\":0},\n \"jars\": [\"s3://c1-data-lake/jars/postgresql-42.3.1.jar\",\"s3://c1-data-lake/jars/mongo-java-driver-3.4.2.jar\",\"s3://c1-data-lake/jars/mongodb-driver-core-3.4.2.jar\",\"s3://c1-data-lake/jars/mongo-spark-connector_2.11-2.2.1.jar\",\"s3://c1-data-lake/jars/bson-3.4.2.jar\"]}","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T09:40:46.210286Z","ended_at":"2024-05-16T09:40:46.292045Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_API_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def clearSparkCache(spark,sqlCtx):\n    sqlCtx.clearCache()\n    spark.catalog.clearCache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMongoDataBySpark(spark,collection,pipeline):\n    client_collection=spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n                .option(\"uri\",etlconfig.mongo_uri+\"\u0026connectTimeoutMS=900000\") \\\n                .option(\"batchSize\",\"50000\") \\\n                .option(\"collection\",collection).option(\"pipeline\",pipeline).load()\n    \n    return client_collection","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T05:57:24.998887Z","ended_at":"2024-05-16T05:57:25.068128Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def getClientDataFromMongo(spark,batch_id,oem_code,mongo_query,delta_date,natural_key_list):    \n    nat_key_df_str = '\",\"'.join(natural_key_list)\n    nat_key_df_str = '\"' +nat_key_df_str+ '\"'\n    oem_collection_code=\"\"\n    if (oem_code.lower()) in ['lexus','gm','kia','bmw','mini','mas','rry','mtr','mtb','fc','fca','dd','dd-bmw']:\n        oem_collection_code = oem_code.lower() + \"_vehicle360\"\n    else: \n        oem_collection_code = \"dd_vehicle360\"\n    \n    dynamic_query = str(mongo_query).replace(\"{dealer_code_array}\",nat_key_df_str).replace(\"{date}\",str(delta_date))\n#     print(dynamic_query)\n    return getMongoDataBySpark(spark,oem_collection_code,dynamic_query)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getClientDataFromMongo(spark,batch_id,oem_code,mongo_query,delta_date,natural_key_list):  \n    dynamic_query = str(mongo_query).replace(\"{dealer_code_array}\",nat_key_df_str).replace(\"{date}\",str(delta_date))\n    return dynamic_query","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T06:12:28.643845Z","ended_at":"2024-05-16T06:12:28.718504Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_CLIENT_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def sparkReadTextFile(spark,sc,src_path):    \n    dest_path=\"s3n://\"+etlconfig.s3_bucket_name+\"/\"+src_path    \n    rdf=spark.read.text(dest_path)\n    return rdf","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T09:07:19.154595Z","ended_at":"2024-05-16T09:07:19.225236Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_API_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def sparkreadtext(spark,textfile_path):\n    textfile_path=\"C:/Users/anjali.3/Downloads/anjali.csv\"\n    rdf=spark.read.csv(textfile_path)\n    return rdf\n\n    ","metadata":{"qubole":{"execution_info":{"started_at":"2024-05-16T10:05:05.178822Z","ended_at":"2024-05-16T10:05:05.249510Z","user_email":"agaur@affinitiv.com","cluster":"ADMT_API_PROD_CLUSTER","clusterType":"cluster"}},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n \nspark = SparkSession.builder.appName(\n    'Read All CSV Files in Directory').getOrCreate()\n \nfile2 = spark.read.csv('/content/*.csv', sep=',', \n                    inferSchema=True, header=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createClientFileByPartition(client_df,dest_path,dynamic_query,file_type_id,delimiter,partition,spark,sc):\n    try:\n        client_df.createOrReplaceTempView(\"clientfile\")\n        print(dest_path)        \n#       // add natrual key to dynamic query to partition by natural key\n\n        if(file_type_id==1):\n            dynamic_query=dynamic_query.replace(\"from\",\" ,client_id from \").replace(\"group by\",\"group by client_id, \")\\\n            .replace(\"CUSTOMER_CHECKSUM from (\",\"CUSTOMER_CHECKSUM,client_id from (\")\n\n        if( file_type_id==2 or file_type_id==3 or file_type_id==5):\n            print(\"211\")\n            dynamic_query=dynamic_query.replace(\"from clientfile\",\" ,client_id from clientfile\").replace(\"group by\",\"group by client_id, \")\\\n            .replace(\"CUSTOMER_CHECKSUM from (\",\"CUSTOMER_CHECKSUM,client_id from (\")\n            print(\"213\")\n        elif(file_type_id==21):\n            partition=\"1\"\n            opcodekeywords = S3filepathtoDataframe(\"ETL/PROD/OpcodeFiles/opcodeid_to_desc_all.txt\",\"\\t\",\"infer\")\n            opcodekeywordsfinal = opcodekeywords[['opcode_category_id','category_name']]\n            opcodekeywordsfinal['opcode_category_id'] = pd.to_numeric(opcodekeywordsfinal['opcode_category_id'], downcast='integer')\n            ref_opcode=spark.createDataFrame(opcodekeywordsfinal)\n            ref_opcode.createOrReplaceTempView(\"opcodekeywordsfinal\")\n            \n#       // execute the dynamic query to get final data\n        finaldataframe = spark.sql(dynamic_query)\n        #print(dynamic_query)\n        #print(finaldataframe)\n        print(finaldataframe.count())\n#         finaldataframe.count()\n#       // write result data into S3 by partition\n        sparkCsvWriteByPartition(sc,finaldataframe,partition,delimiter,dest_path)\n        finaldataframe.unpersist()\n    except Exception as ex:\n        print(ex)\n        \n","metadata":{},"execution_count":null,"outputs":[]}]}